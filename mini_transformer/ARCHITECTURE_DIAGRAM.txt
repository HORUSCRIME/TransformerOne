╔══════════════════════════════════════════════════════════════════════════════╗
║                    MINI TRANSFORMER ARCHITECTURE                             ║
╚══════════════════════════════════════════════════════════════════════════════╝

┌──────────────────────────────────────────────────────────────────────────────┐
│                              INPUT TOKENS                                    │
│                         [batch_size, seq_len]                                │
└────────────────────────────────┬─────────────────────────────────────────────┘
                                 │
                                 ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                         TOKEN EMBEDDING                                      │
│                    [batch_size, seq_len, d_model]                            │
│                                                                              │
│  • Converts token IDs to dense vectors                                      │
│  • Learnable embedding matrix [vocab_size, d_model]                         │
└────────────────────────────────┬─────────────────────────────────────────────┘
                                 │
                                 ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                            DROPOUT                                           │
└────────────────────────────────┬─────────────────────────────────────────────┘
                                 │
                                 ▼
╔══════════════════════════════════════════════════════════════════════════════╗
║                      TRANSFORMER BLOCK (x N layers)                          ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                         LAYER NORM (PreNorm)                           │ ║
║  └──────────────────────────────┬─────────────────────────────────────────┘ ║
║                                 │                                            ║
║                                 ▼                                            ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                   MULTI-HEAD SELF-ATTENTION                            │ ║
║  │                                                                        │ ║
║  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐                │ ║
║  │  │   Q Proj     │  │   K Proj     │  │   V Proj     │                │ ║
║  │  │  [d → d]     │  │  [d → d]     │  │  [d → d]     │                │ ║
║  │  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘                │ ║
║  │         │                  │                  │                        │ ║
║  │         ▼                  ▼                  ▼                        │ ║
║  │  ┌──────────────────────────────────────────────────┐                 │ ║
║  │  │         Apply RoPE to Q and K                    │                 │ ║
║  │  │  (Rotary Position Embeddings)                    │                 │ ║
║  │  └──────────────────┬───────────────────────────────┘                 │ ║
║  │                     │                                                  │ ║
║  │                     ▼                                                  │ ║
║  │  ┌──────────────────────────────────────────────────┐                 │ ║
║  │  │         Attention(Q, K, V)                       │                 │ ║
║  │  │  score = (Q @ K^T) / sqrt(d_k)                  │                 │ ║
║  │  │  score = masked_fill(causal_mask)               │                 │ ║
║  │  │  attn = softmax(score)                          │                 │ ║
║  │  │  out = attn @ V                                 │                 │ ║
║  │  └──────────────────┬───────────────────────────────┘                 │ ║
║  │                     │                                                  │ ║
║  │                     ▼                                                  │ ║
║  │  ┌──────────────────────────────────────────────────┐                 │ ║
║  │  │         Output Projection                        │                 │ ║
║  │  └──────────────────┬───────────────────────────────┘                 │ ║
║  └────────────────────┬────────────────────────────────────────────────┘ ║
║                       │                                                    ║
║                       ▼                                                    ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                    RESIDUAL CONNECTION                                 │ ║
║  │                    x = x + attention(x)                                │ ║
║  └──────────────────────────────┬─────────────────────────────────────────┘ ║
║                                 │                                            ║
║                                 ▼                                            ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                         LAYER NORM (PreNorm)                           │ ║
║  └──────────────────────────────┬─────────────────────────────────────────┘ ║
║                                 │                                            ║
║                                 ▼                                            ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                         SWIGLU FFN                                     │ ║
║  │                                                                        │ ║
║  │  ┌──────────────┐  ┌──────────────┐                                  │ ║
║  │  │   W1 (gate)  │  │   W2 (up)    │                                  │ ║
║  │  │  [d → 4d]    │  │  [d → 4d]    │                                  │ ║
║  │  └──────┬───────┘  └──────┬───────┘                                  │ ║
║  │         │                  │                                          │ ║
║  │         ▼                  ▼                                          │ ║
║  │  ┌──────────────────────────────────┐                                │ ║
║  │  │  SiLU(W1(x)) * W2(x)             │                                │ ║
║  │  └──────────────┬───────────────────┘                                │ ║
║  │                 │                                                     │ ║
║  │                 ▼                                                     │ ║
║  │  ┌──────────────────────────────────┐                                │ ║
║  │  │   W3 (down projection)           │                                │ ║
║  │  │   [4d → d]                       │                                │ ║
║  │  └──────────────┬───────────────────┘                                │ ║
║  └────────────────┬────────────────────────────────────────────────────┘ ║
║                   │                                                        ║
║                   ▼                                                        ║
║  ┌────────────────────────────────────────────────────────────────────────┐ ║
║  │                    RESIDUAL CONNECTION                                 │ ║
║  │                    x = x + ffn(x)                                      │ ║
║  └────────────────────────────────────────────────────────────────────────┘ ║
║                                                                              ║
╚════════════════════════════════┬═════════════════════════════════════════════╝
                                 │
                                 │ (Repeat N times)
                                 │
                                 ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                         FINAL LAYER NORM                                     │
└────────────────────────────────┬─────────────────────────────────────────────┘
                                 │
                                 ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                         LM HEAD (Linear)                                     │
│                    [d_model → vocab_size]                                    │
│                                                                              │
│  • Projects to vocabulary size                                              │
│  • Weight-tied with token embedding                                         │
└────────────────────────────────┬─────────────────────────────────────────────┘
                                 │
                                 ▼
┌──────────────────────────────────────────────────────────────────────────────┐
│                         OUTPUT LOGITS                                        │
│                  [batch_size, seq_len, vocab_size]                           │
└──────────────────────────────────────────────────────────────────────────────┘


╔══════════════════════════════════════════════════════════════════════════════╗
║                         KEY FEATURES                                         ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  ✓ RoPE (Rotary Position Embeddings)                                        ║
║    • Better than learned/sinusoidal embeddings                              ║
║    • Generalizes to longer sequences                                        ║
║    • Encodes relative positions                                             ║
║                                                                              ║
║  ✓ PreNorm (Pre-Layer Normalization)                                        ║
║    • Normalizes before attention/FFN                                        ║
║    • Better training stability                                              ║
║    • Enables deeper models                                                  ║
║                                                                              ║
║  ✓ SWIGLU Activation                                                        ║
║    • SiLU(W1(x)) * W2(x) @ W3                                               ║
║    • Better than ReLU/GELU                                                  ║
║    • Used in modern LLMs                                                    ║
║                                                                              ║
║  ✓ Causal Masking                                                           ║
║    • Decoder-only architecture                                              ║
║    • Prevents attending to future tokens                                    ║
║    • Autoregressive generation                                              ║
║                                                                              ║
║  ✓ Weight Tying                                                             ║
║    • Embedding and LM head share weights                                    ║
║    • Reduces parameters                                                     ║
║    • Better generalization                                                  ║
║                                                                              ║
║  ✓ KV-Cache (for generation)                                                ║
║    • Cache key/value tensors                                                ║
║    • Only compute attention for new token                                   ║
║    • 10x faster generation                                                  ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝


╔══════════════════════════════════════════════════════════════════════════════╗
║                    GENERATION FLOW (with KV-Cache)                           ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  Step 1: Process prompt                                                     ║
║  ┌────────────────────────────────────────────────────────────────────┐    ║
║  │  Input: "Once upon a"                                              │    ║
║  │  Forward pass → Cache K, V for all tokens                          │    ║
║  │  Output: logits for next token                                     │    ║
║  └────────────────────────────────────────────────────────────────────┘    ║
║                                                                              ║
║  Step 2: Generate token 1                                                   ║
║  ┌────────────────────────────────────────────────────────────────────┐    ║
║  │  Sample: "time"                                                    │    ║
║  │  Forward pass (only new token) → Append to K, V cache             │    ║
║  │  Output: logits for next token                                     │    ║
║  └────────────────────────────────────────────────────────────────────┘    ║
║                                                                              ║
║  Step 3: Generate token 2                                                   ║
║  ┌────────────────────────────────────────────────────────────────────┐    ║
║  │  Sample: "there"                                                   │    ║
║  │  Forward pass (only new token) → Append to K, V cache             │    ║
║  │  Output: logits for next token                                     │    ║
║  └────────────────────────────────────────────────────────────────────┘    ║
║                                                                              ║
║  ... Continue until max_length or EOS token                                 ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝


╔══════════════════════════════════════════════════════════════════════════════╗
║                         PARAMETER COUNTS                                     ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  Token Embedding:     vocab_size × d_model                                  ║
║  Per Layer:           ~12 × d_model²                                        ║
║    - Q, K, V proj:    3 × d_model²                                          ║
║    - Output proj:     d_model²                                              ║
║    - FFN:             8 × d_model² (SWIGLU has 3 matrices)                 ║
║  Total:               vocab_size × d_model + n_layers × 12 × d_model²      ║
║                                                                              ║
║  Example (default config):                                                  ║
║    vocab_size = 100, d_model = 256, n_layers = 6                           ║
║    Total ≈ 2.6M parameters                                                  ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝


╔══════════════════════════════════════════════════════════════════════════════╗
║                         DATA FLOW DIMENSIONS                                 ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                              ║
║  Input:               [B, T]                                                ║
║  After Embedding:     [B, T, D]                                             ║
║  After Attention:     [B, T, D]                                             ║
║  After FFN:           [B, T, D]                                             ║
║  After LM Head:       [B, T, V]                                             ║
║                                                                              ║
║  Where:                                                                     ║
║    B = batch_size                                                           ║
║    T = sequence_length (block_size)                                         ║
║    D = d_model                                                              ║
║    V = vocab_size                                                           ║
║                                                                              ║
╚══════════════════════════════════════════════════════════════════════════════╝
